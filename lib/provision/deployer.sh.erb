#!/bin/bash

cat >> /etc/resolv.conf <<EOF
nameserver 192.168.121.1
EOF

# remove all the repos!
zypper lr | awk '{ print $3 }' | xargs zypper rr

zypper ar <%= opensuse_oss_url %> opensuse_oss
zypper ar <%= opensuse_nonoss_url %> opensuse_nonoss
zypper ar <%= opensuse_updates_oss_url %> opensuse_updates_oss
zypper ar <%= opensuse_updates_nonoss_url %> opensuse_updates_nonoss
zypper refresh
zypper --gpg-auto-import-keys refresh
zypper -n update

zypper -n in git python-devel which jq ipcalc python3-pip python-xml docker
zypper -n in -t pattern devel_basis

# the socok8s playbooks will do this, but it will cause a failure becuase the vagrant user
# will still not have permission to talk to the docker daemon. this way we will
gpasswd -a vagrant docker
systemctl enable --now docker

# helm will later need access to our kubeconfig
ln -s /home/vagrant/.kube /root/.kube

# virtualenv from the opensuse repos are broken :'(
pip install virtualenv

cat >> /etc/hosts <<EOF
<%- for host in hosts do -%>
<%= host[:ip] %> <%= host[:hostname] %>
<%- end -%>
EOF
cd /home/vagrant
sudo -u vagrant git clone --recursive https://github.com/SUSE-Cloud/socok8s
cd -

sudo -u vagrant mkdir -p /home/vagrant/dev-workspace/inventory
sudo -u vagrant mkdir -p /home/vagrant/dev-workspace/env

sudo -u vagrant cat >> /home/vagrant/.bashrc <<EOF
export SOCOK8S_USE_VIRTUALENV=True
export SOCOK8S_ENVNAME=dev
EOF

sudo -u vagrant cat >> /home/vagrant/.bash_profile <<EOF
export SOCOK8S_USE_VIRTUALENV=True
export SOCOK8S_ENVNAME=dev
EOF

cd /home/vagrant/socok8s
sudo -u vagrant git apply - <<EOF
diff --git a/playbooks/roles/airship-deploy-osh/tasks/main.yml b/playbooks/roles/airship-deploy-osh/tasks/main.yml
index 8314a66..e2f374a 100644
--- a/playbooks/roles/airship-deploy-osh/tasks/main.yml
+++ b/playbooks/roles/airship-deploy-osh/tasks/main.yml
@@ -178,7 +178,7 @@
     OS_PASSWORD: "{{ lookup('password', secrets_location + '/ucp_shipyard_keystone_password ' + password_opts) }}"
   register: shipyard_desc_action
   until: shipyard_desc_action.stdout.find('Processing') < 0 and shipyard_desc_action.stdout.find('running') < 0
-  retries: 240
+  retries: 480
   delay: 15
   ignore_errors: yes
   tags:
diff --git a/playbooks/roles/airship-deploy-ucp/tasks/main.yml b/playbooks/roles/airship-deploy-ucp/tasks/main.yml
index 0ec5a78..d442863 100644
--- a/playbooks/roles/airship-deploy-ucp/tasks/main.yml
+++ b/playbooks/roles/airship-deploy-ucp/tasks/main.yml
@@ -206,7 +206,7 @@
   shell: "kubectl get pod {{ armada_pod_name }} -n {{ ucp_namespace_name }} -o jsonpath='{.status.containerStatuses[].ready}'"
   register: armada_pod_status
   until: armada_pod_status.stdout == "true"
-  retries: 30
+  retries: 60
   delay: 10
   ignore_errors: yes
   tags:
@@ -258,7 +258,7 @@
   command: 'kubectl get pod -l application=armada,component=api -n {{ ucp_namespace_name }} -o jsonpath="{.items[0].metadata.name}"'
   register: armada_results
   until: armada_results.stdout.find('armada-api-') == 0
-  retries: 180
+  retries: 360
   delay: 10
   ignore_errors: yes
   tags:
@@ -275,7 +275,7 @@
   command: "kubectl get pod {{ armada_api_pod_name }} -n {{ ucp_namespace_name }} -o jsonpath='{.status.containerStatuses[].ready}'"
   register: armada_api_pod_status
   until: armada_api_pod_status.stdout == "true"
-  retries: 60
+  retries: 120
   delay: 10
   ignore_errors: yes
   tags:
diff --git a/playbooks/roles/common-deployer/tasks/helm-install.yml b/playbooks/roles/common-deployer/tasks/helm-install.yml
index 9c2bc08..77b5f89 100644
--- a/playbooks/roles/common-deployer/tasks/helm-install.yml
+++ b/playbooks/roles/common-deployer/tasks/helm-install.yml
@@ -40,12 +40,14 @@
     - install
 
 - name: Install tiller
+  become: yes
   command: helm init
   register: _helminstall
   when: helm_state == 'to_install'
   changed_when: True
 
 - name: Upgrade tiller
+  become: yes
   command: helm init --upgrade
   register: _helmupgrade
   when: helm_state == 'to_update' or helm_state == 'to_install'
diff --git a/playbooks/roles/common-deployer/tasks/helm-run.yml b/playbooks/roles/common-deployer/tasks/helm-run.yml
index 964a8ad..a59f343 100644
--- a/playbooks/roles/common-deployer/tasks/helm-run.yml
+++ b/playbooks/roles/common-deployer/tasks/helm-run.yml
@@ -40,6 +40,7 @@
     executable: /bin/bash
 
 - name: Adding helm repos
+  become: yes
   command: helm repo add localhost http://localhost:8879/charts
   when:
     - _helmrepolist.rc != 0
EOF
cd -

cd /home/vagrant/socok8s/files
sudo -u vagrant tar xf ./velum-bootstrap.tar.xz
cd -
cd /home/vagrant/socok8s/files/velum-bootstrap
./velum-interactions --setup
cd -

sudo -u vagrant cat >> /home/vagrant/socok8s/files/velum-bootstrap/environment.json <<EOF
{
    "minions": [
    <%- for host in hosts.filter { |h| %w{admin master worker}.include?(h[:type]) } -%>
      {
        "minionId": "<%= host[:hostname] %>",
        "role": "<%= host[:type] %>"
      }
    <%- end -%>
    ],
    "kubernetesExternalHost": "caasp-master-1",
    "dashboardExternalHost": "caasp-admin"
}
EOF

sudo -u vagrant cat >> /home/vagrant/post-install.sh <<EOF
#!/bin/bash

cd /home/vagrant

cd /home/vagrant/socok8s/files/velum-bootstrap
./velum-interactions -c -b -k --enable-tiller -e ./environment.json

cd /home/vagrant
./copy_kubernetes_config.sh

cd ./socok8s/

./run.sh deploy_ses
./run.sh setup_caasp_workers_for_openstack
./run.sh deploy
EOF
chmod +x /home/vagrant/post-install.sh


sudo -u vagrant cat >> /home/vagrant/dev-workspace/env/extravars <<EOF
ses_repo_server: "<%= clouddata_mirror %>"
socok8s_ext_vip: 192.168.121.2
socok8s_dcm_vip: 192.168.121.3
deploy_on_openstack_repos_to_configure:
  SLE12SP3-product: "<%= clouddata_mirror %>/repos/x86_64/SLES12-SP3-Pool/"
  SLE12SP3-update: "<%= clouddata_mirror %>/repos/x86_64/SLES12-SP3-Updates/"
  SLE12SP3SDK-product: "<%= clouddata_mirror %>/repos/x86_64/SLE12-SP3-SDK-Pool/"
  SLE12SP3SDK-update: "<%= clouddata_mirror %>/repos/x86_64/SLE12-SP3-SDK-Updates/"
  SUSE-CA: "<%= ibs_mirror %>/dist/ibs/SUSE:/CA/SLE_12_SP3/"
  OpenStack-Cloud8-product: "<%= clouddata_mirror %>/repos/x86_64/SUSE-OpenStack-Cloud-8-Pool/"
  SES5-product: "<%= clouddata_mirror %>/repos/x86_64/SUSE-Enterprise-Storage-5-Pool/"
  SES5-update: "<%= clouddata_mirror %>/repos/x86_64/SUSE-Enterprise-Storage-5-Updates/"
  SLE12Containers: "<%= ibs_mirror %>/ibs/SUSE/Updates/SLE-Module-Containers/12/x86_64/update/"
  Leap15develtools: "https://download.opensuse.org/repositories/devel:/tools/openSUSE_Leap_15.0/"
  CAASP30-update: "<%= ibs_mirror %>/ibs/SUSE/Updates/SUSE-CAASP/3.0/x86_64/update/"
  CAASP30-pool: "<%= ibs_mirror %>/ibs/SUSE/Products/SUSE-CAASP/3.0/x86_64/product/"
EOF

sudo -u vagrant cat >> /home/vagrant/dev-workspace/inventory/default-inventory.yml <<EOF
---
caasp-admin:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('admin') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

caasp-masters:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('master') %>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

caasp-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

soc-deployer:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('deployer') -%>
    <%= host[:hostname] %>:
      ansible_connection: local
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

ses_nodes:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('ses') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-openstack-compute-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-openstack-control-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-ucp-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-kube-system-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root
EOF

sudo -u vagrant mkdir -p /home/vagrant/.ssh/

sudo -u vagrant cat >> /home/vagrant/.ssh/config <<EOF
<%- for host in hosts do -%>
<%- if host[:type] != 'deployer' -%>
Host <%= host[:hostname] %>
  Hostname <%= host[:ip] %>
  User root
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentitiesOnly yes
<%- end -%>
<%- end -%>
EOF

sudo -u vagrant cat >> /home/vagrant/.ssh/id_rsa <<EOF
<%= deployer_private_key %>EOF

sudo -u vagrant cat >> /home/vagrant/.ssh/id_rsa.pub <<EOF
<%= deployer_public_key %>EOF

sudo -u vagrant cat >> /home/vagrant/.alias <<EOF
export SOCOK8S_ENVNAME=dev
export DEPLOYMENT_MECHANISM='kvm'
EOF

sudo -u vagrant virtualenv /home/vagrant/dev-workspace/.ansiblevenv
sudo -u vagrant bash - <<EOF
source /home/vagrant/dev-workspace/.ansiblevenv/bin/activate
pip install -r /home/vagrant/socok8s/script_library/requirements.txt
EOF

sudo -u vagrant cat >> /home/vagrant/copy_kubernetes_config.sh <<EOF
#!/bin/bash
mkdir -p /home/vagrant/.kube
ssh caasp-admin -C 'sudo cat /root/.kube/config' > /home/vagrant/.kube/config 2> /dev/null
ssh caasp-admin -C 'sudo cat /etc/pki/trust/anchors/SUSE_CaaSP_CA.crt' > /home/vagrant/.kube/SUSE_CaaSP_CA.crt 2> /dev/null
ssh caasp-admin -C 'sudo cat /etc/pki/kubectl-client-cert.crt' > /home/vagrant/.kube/kubectl-client-cert.crt 2> /dev/null
ssh caasp-admin -C 'sudo cat /etc/pki/kubectl-client-cert.key' > /home/vagrant/.kube/kubectl-client-cert.key 2> /dev/null

sed -i 's/\/etc\/pki\/trust\/anchors/\/home\/vagrant\/.kube/' /home/vagrant/.kube/config
sed -i 's/\/etc\/pki/\/home\/vagrant\/.kube/' /home/vagrant/.kube/config
sed -i 's/api\.infra\.caasp\.local/caasp-master-1/' /home/vagrant/.kube/config
EOF

chmod +x /home/vagrant/copy_kubernetes_config.sh

curl -sLO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
mv ./kubectl /usr/local/bin/kubectl

chown -R vagrant:users /home/vagrant
chmod 400 /home/vagrant/.ssh/id_rsa

systemctl disable -now firewalld.service
