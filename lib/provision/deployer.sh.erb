#!/bin/bash

# remove all the repos!
zypper lr | awk '{ print $3 }' | xargs zypper rr

zypper ar <%= opensuse_oss_url %> opensuse_oss
zypper ar <%= opensuse_nonoss_url %> opensuse_nonoss
zypper ar <%= opensuse_updates_oss_url %> opensuse_updates_oss
zypper ar <%= opensuse_updates_nonoss_url %> opensuse_updates_nonoss
zypper refresh
zypper -n update

cat >> /etc/resolv.conf <<EOF
nameserver 192.168.121.1
EOF

zypper -n in git python-devel which jq ipcalc python3-pip python-xml docker
zypper -n in -t pattern devel_basis

# the socok8s playbooks will do this, but it will cause a failure becuase the vagrant user
# will still not have permission to talk to the docker daemon. this way we will
gpasswd -a vagrant docker
systemctl enable --now docker

# helm will later need access to our kubeconfig
ln -s /home/vagrant/.kube /root/.kube

# virtualenv from the opensuse repos are broken :'(
pip install virtualenv

cat >> /etc/hosts <<EOF
<%- for host in hosts do -%>
<%= host[:ip] %> <%= host[:hostname] %>
<%- end -%>
EOF
cd /home/vagrant
sudo -u vagrant git clone --recursive https://github.com/SUSE-Cloud/socok8s

sudo -u vagrant mkdir -p /home/vagrant/dev-workspace/inventory
sudo -u vagrant mkdir -p /home/vagrant/dev-workspace/env

sudo -u vagrant cat >> /home/vagrant/.bashrc <<EOF
export SOCOK8S_USE_VIRTUALENV=True
export SOCOK8S_ENVNAME=dev
EOF

sudo -u vagrant cat >> /home/vagrant/.bash_profile <<EOF
export SOCOK8S_USE_VIRTUALENV=True
export SOCOK8S_ENVNAME=dev
EOF

sudo -u vagrant cat >> /home/vagrant/dev-workspace/env/extravars <<EOF
ses_repo_server: "<%= clouddata_mirror %>"
deploy_on_openstack_repos_to_configure:
  SLE12SP3-product: "<%= clouddata_mirror %>/repos/x86_64/SLES12-SP3-Pool/"
  SLE12SP3-update: "<%= clouddata_mirror %>/repos/x86_64/SLES12-SP3-Updates/"
  SLE12SP3SDK-product: "<%= clouddata_mirror %>/repos/x86_64/SLE12-SP3-SDK-Pool/"
  SLE12SP3SDK-update: "<%= clouddata_mirror %>/repos/x86_64/SLE12-SP3-SDK-Updates/"
  SUSE-CA: "<%= ibs_mirror %>/dist/ibs/SUSE:/CA/SLE_12_SP3/"
  OpenStack-Cloud8-product: "<%= clouddata_mirror %>/repos/x86_64/SUSE-OpenStack-Cloud-8-Pool/"
  SES5-product: "<%= clouddata_mirror %>/repos/x86_64/SUSE-Enterprise-Storage-5-Pool/"
  SES5-update: "<%= clouddata_mirror %>/repos/x86_64/SUSE-Enterprise-Storage-5-Updates/"
  SLE12Containers: "<%= ibs_mirror %>/ibs/SUSE/Updates/SLE-Module-Containers/12/x86_64/update/"
  Leap15develtools: "https://download.opensuse.org/repositories/devel:/tools/openSUSE_Leap_15.0/"
  CAASP30-update: "<%= ibs_mirror %>/ibs/SUSE/Updates/SUSE-CAASP/3.0/x86_64/update/"
  CAASP30-pool: "<%= ibs_mirror %>/ibs/SUSE/Products/SUSE-CAASP/3.0/x86_64/product/"
EOF

sudo -u vagrant cat >> /home/vagrant/dev-workspace/inventory/default-inventory.yml <<EOF
---
caasp-admin:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('admin') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

caasp-masters:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('master') %>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

caasp-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

soc-deployer:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('deployer') -%>
    <%= host[:hostname] %>:
      ansible_connection: local
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

ses_nodes:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('ses') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-openstack-compute-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-openstack-control-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-ucp-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root

airship-kube-system-workers:
  hosts:
<%- for host in hosts do -%>
  <%- if host[:hostname].include?('worker') -%>
    <%= host[:hostname] %>:
  <%- end -%>
<%- end -%>
  vars:
    ansible_user: root
EOF

sudo -u vagrant mkdir -p /home/vagrant/.ssh/

sudo -u vagrant cat >> /home/vagrant/.ssh/config <<EOF
<%- for host in hosts do -%>
<%- if host[:type] != 'deployer' -%>
Host <%= host[:hostname] %>
  Hostname <%= host[:ip] %>
  User root
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
  PasswordAuthentication no
  IdentitiesOnly yes
<%- end -%>
<%- end -%>
EOF

sudo -u vagrant cat >> /home/vagrant/.ssh/id_rsa <<EOF
<%= deployer_private_key %>EOF

sudo -u vagrant cat >> /home/vagrant/.ssh/id_rsa.pub <<EOF
<%= deployer_public_key %>EOF

sudo -u vagrant cat >> /home/vagrant/.alias <<EOF
export SOCOK8S_ENVNAME=dev
export DEPLOYMENT_MECHANISM='kvm'
EOF

sudo -u vagrant virtualenv /home/vagrant/dev-workspace/.ansiblevenv
sudo -u vagrant bash - <<EOF
source /home/vagrant/dev-workspace/.ansiblevenv/bin/activate
pip install -r /home/vagrant/socok8s/script_library/requirements.txt
EOF

sudo -u vagrant cat >> /home/vagrant/copy_kubernetes_config.sh <<EOF
#!/bin/bash
mkdir -p /home/vagrant/.kube
ssh caasp-admin -C 'sudo cat /root/.kube/config' > /home/vagrant/.kube/config 2> /dev/null
ssh caasp-admin -C 'sudo cat /etc/pki/trust/anchors/SUSE_CaaSP_CA.crt' > /home/vagrant/.kube/SUSE_CaaSP_CA.crt 2> /dev/null
ssh caasp-admin -C 'sudo cat /etc/pki/kubectl-client-cert.crt' > /home/vagrant/.kube/kubectl-client-cert.crt 2> /dev/null
ssh caasp-admin -C 'sudo cat /etc/pki/kubectl-client-cert.key' > /home/vagrant/.kube/kubectl-client-cert.key 2> /dev/null

sed -i 's/\/etc\/pki\/trust\/anchors/\/home\/vagrant\/.kube/' /home/vagrant/.kube/config
sed -i 's/\/etc\/pki/\/home\/vagrant\/.kube/' /home/vagrant/.kube/config
sed -i 's/api\.infra\.caasp\.local/caasp-master-1/' /home/vagrant/.kube/config
EOF

chmod +x /home/vagrant/copy_kubernetes_config.sh

curl -sLO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
mv ./kubectl /usr/local/bin/kubectl

chown -R vagrant:users /home/vagrant
chmod 400 /home/vagrant/.ssh/id_rsa

systemctl disable firewalld.service

reboot
